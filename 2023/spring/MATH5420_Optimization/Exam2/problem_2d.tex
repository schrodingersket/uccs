Confirm this result for the function 

$$
f(x) = 3 x_1^2 - 2 x_1 x_2 + 3 x_2^2 + 2 x_1 - 6 x_2.
$$

Use $x_0 = (1, 2)^T$ as a starting point for the method and compute the point obtained by one iteration of the 
steepest-descent algorithm. Prove that this point is the unique minimum $x_*$ and verify that $x_0 - x_*$ is an
eigenvector of the Hessian matrix.

\begin{solution}
    We begin by writing our objective function in quadratic form, which yields

    $$
    f(x) = \frac{1}{2} x^T Q x + c^T x, \quad Q = \begin{pmatrix*}[r]
        6 & -2 \\
       -2 &  6
    \end{pmatrix*}, \quad \text{and} \quad c = \begin{pmatrix*}[r]
        2 \\
       -6
    \end{pmatrix*}.
    $$

    Our gradient and Hessian are therefore

    $$
    \nabla f(x) = Q x + c = \begin{pmatrix*}[r]
        6 x_1 - 2 x_2 + 2 \\
       -2 x_1 + 6 x_2 - 6
    \end{pmatrix*} \quad \text{and} \quad \nabla^2 f(x) = Q = \begin{pmatrix*}[r]
        6 & -2 \\
       -2 &  6
    \end{pmatrix*},
    $$

    respectively. The first iteration of the steepest-descent algorithm yields \linebreak
    $x_1 = x_0 + \alpha_0 p_0$, where at $x_0 = (1, 2)^T$ we have\footnote{
        Matrix computations are performed in \texttt{problem\_2d.py}.
    }

    $$
    p_0 = -\nabla f(x_0) = \begin{pmatrix*}[r]
        -4 \\
        -4
    \end{pmatrix*}
    $$

    and 

    $$
    \alpha_0 = -\frac{\nabla f(x_0) p_0}{p_0^T Q p_0} = \frac{32}{128} = \frac{1}{4}
    $$

    so that

    $$
    x_1 = x_0 + \alpha_0 p_0 = \begin{pmatrix*}[r]
        1 \\
        2
    \end{pmatrix*} + \frac{1}{4} \begin{pmatrix*}[r]
        -4 \\
        -4
    \end{pmatrix*} = \begin{pmatrix*}[r]
        0 \\
        1
    \end{pmatrix*}.
    $$

    To confirm the results of (c), we begin with the observation that the first-order sufficient conditions for $x_1$ to be a minimizer of 
    $f$ are satisfied, since

    $$
    \nabla f(x_1) = \begin{pmatrix*}[r]
        -2 + 2 \\
         6 - 6
    \end{pmatrix*} = \begin{pmatrix*}
        0 \\
        0
    \end{pmatrix*}.
    $$

    Moreover, $\nabla^2 f(x_1) = Q$ is diagonally dominant and hence positive definite (with eigenvalues 
    $\lambda_1 = 4, \lambda_2 = 8$) so that $x_1 = x_*$ is a minimizer for $f$, as desired.
    \pagebreak

    We confirm the results of (a) with the observation that

    $$
    x_* - x_0 = x_1 - x_0 = \begin{pmatrix*}[r]
        0 \\
        1
    \end{pmatrix*} - \begin{pmatrix*}[r]
        1 \\
        2
    \end{pmatrix*} = \begin{pmatrix*}[r]
        -1 \\
        -1
    \end{pmatrix*}
    $$

    and hence

    \begin{align*}
        Q(x_* - x_0) &= \begin{pmatrix*}[r]
                            6 & -2 \\
                            -2 &  6
                        \end{pmatrix*} \begin{pmatrix*}
                            -1 \\
                            -1
                        \end{pmatrix*} \\
                     &= \begin{pmatrix*}[r]
                            -4 \\
                            -4
                        \end{pmatrix*} \\
                     &= 4 \begin{pmatrix*}[r]
                            -1 \\
                            -1
                        \end{pmatrix*} \\
                     &= \lambda_1 (x_* - x_0).
    \end{align*}

    The vector $x_* - x_0$ is therefore an eigenvector of $Q$ as predicted by (a); we conclude with the observation that
    $\alpha_0 = \frac{1}{4} = \frac{1}{\lambda_1}$ where $\lambda_1$ is the eigenvalue associated with $x_* - x_0$, as 
    predicted by (b).
    \ \\
\end{solution}