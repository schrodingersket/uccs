\textbf{Griva, Nash, Sofer 12.3.1}

Apply the symmetric rank-one quasi-Newton method to solve

\begin{mini*}
    {x \in \mathbb{R}^3}{f(x) = \frac{1}{2} x^T Q x - c^T x}{}{}
\end{mini*}

with

$$
Q = \begin{pmatrix*}[r]
    5 & 2 & 1 \\
    2 & 7 & 3 \\
    1 & 3 & 9
\end{pmatrix*} \quad \text{and} \quad c = \begin{pmatrix*}[r]
    -9 \\
     0 \\
    -8
\end{pmatrix*}.
$$

\textbf{Note:} Write code that utilizes inverse quasi-Hessian approximations in 12.3.1 and 12.3.2.

\begin{solution}
    At the initial point $x_0$, the gradient and its norm are given by

    $$
    \nabla f(x_0) = \begin{pmatrix*}[r]
         9 \\
         0 \\
         8
    \end{pmatrix*} \quad \text{and} \quad \| \nabla f(x_0) \| = 12.0416, 
    $$

    respectively, and so this point is not optimal. The first search direction $p_0$ is given by the solution to 
    $B_0 p_0 = -\nabla f(x_0)$, where $B_0 = I$ is the initial approximation to the inverse Hessian. Solving for $p_0$ 
    yields:\footnote{
        See \texttt{problem\_6.py} for the code used to solve this problem.
    }

    $$
    p_0 = \begin{pmatrix*}[r]
        -9 \\
         0 \\
        -8
    \end{pmatrix*}
    $$

    and corresponding step length $\alpha_0 = 0.1289$ from the exact line search formula afor a quadratic function.
    The new estimate of the solution, update vectors, and Hessian approximation are 

    $$
    x_1 = \begin{pmatrix*}[r]
        -1.1600 \\
         0.0000 \\
        -1.0311
    \end{pmatrix*}, \quad \nabla f(x_1) = \begin{pmatrix*}[r]
         2.1689 \\
        -5.4133 \\
         2.4400
    \end{pmatrix*}, \quad s_0 = \begin{pmatrix*}[r]
        -1.1600 \\
         0.0000 \\
        -1.0311
    \end{pmatrix*}, \quad y_0 = \begin{pmatrix*}[r]
        -6.8311 \\
        -5.4133 \\
        -10.4400
    \end{pmatrix*}
    $$

    and 

    $$
    B_1 = B_0 + \frac{(y_0 - B_0 s_0)(y_0 - B_0 s_0)^T}{(y_0 - B_0 s_0)^T s_0} = \begin{pmatrix*}[r]
        2.9755 & 1.8857 & 3.2776 \\
        1.8857 & 2.8000 & 3.1286 \\
        3.2776 & 3.1286 & 6.4378
    \end{pmatrix*}.
    $$

    At the next iteration, the gradient norm is given by $\| \nabla f(x_1) \| = 6.3215$ and so this point is not 
    optimal. Our next search direction $p_1$ is the solution to $B_1 p_1 = -\nabla f(x_1)$, which yields:

    $$
    p_1 = \begin{pmatrix*}[r]
        -3.5319 \\
         4.1123 \\
         0.1787
    \end{pmatrix*}
    $$

    with corresponding step length $\alpha_1 = 0.2408$. The new estimates of the solution, update vectors, and Hessian 
    are

    $$
    x_2 = \begin{pmatrix*}[r]
        -2.0104 \\
         0.9901 \\
         0.0430
    \end{pmatrix*}, \quad \nabla f(x_2) = \begin{pmatrix*}[r]
        -0.0597 \\
        -0.0542 \\
         0.0672
    \end{pmatrix*}, \quad s_1 = \begin{pmatrix*}[r]
        -0.8504 \\
         0.9901 \\
         0.0430
    \end{pmatrix*}, \quad y_1 = \begin{pmatrix*}[r]
        -2.2286 \\
         5.3591 \\
         2.5072
    \end{pmatrix*}
    $$

    and 

    $$
    B_2 = \begin{pmatrix*}[r]
        3.5002 & 0.6386 & 2.6873 \\
        0.6386 & 5.7642 & 4.5316 \\
        2.6872 & 4.5316 & 7.1018
    \end{pmatrix*}.
    $$

    At the next iterate, the gradient nrom takes the value $\nabla f(x_2) = 0.1049$, and so $x_2$ is not optimal. The
    next search direction $p_2$ is given by the solution to $B_2 p_2 = -\nabla f(x_2)$, which yields:

    $$
    p_2 = \begin{pmatrix*}[r]
         0.0584 \\
         0.0557 \\
        -0.0671
    \end{pmatrix*}
    $$

    with corresponding step length $\alpha_2 = 0.1775$ and from which we obtain the new point and corresponding gradient

    $$
    x_3 = \begin{pmatrix*}[r]
        -2.0000 \\
         1.0000 \\
        -1.0000
    \end{pmatrix*} \quad \nabla f(x_3) = \begin{pmatrix*}[r]
        0.0000 \\
        0.0000 \\
        0.0000
    \end{pmatrix*}.
    $$

    Since $\nabla f(x_3) = 0$, we have found the optimal solution to the problem at \linebreak
    $x^T = (-2, 1, -1)$, which yields the optimal value

    $$
    f(x_3) = -13.
    $$
    \ \\
\end{solution}
