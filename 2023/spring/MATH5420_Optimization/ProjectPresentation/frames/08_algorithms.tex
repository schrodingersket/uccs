\begin{frame}
    \frametitle{Optimization Algorithms}

    Since neural networks have a \textit{lot} of parameters to optimize, we use stochastic gradient descent variations 
    to make the training process possible on modern hardware.

    \bigskip
    \pause

    In stochastic gradient descent, we use random subsets of the training data to compute the gradient of the residual
    in place of the entire set.

    \bigskip
    \pause

    \textbf{Adaptive Moment Estimation (Adam)}
    \ \\
    \bigskip

    [\textbf{TODO:} Describe Hessian approximation ]

    \pause
    \bigskip

    \textbf{Limited-Memory BFGS (L-BFGS)}
    \ \\
    \bigskip
    [\textbf{TODO:} Describe Hessian approximation ]
\end{frame}