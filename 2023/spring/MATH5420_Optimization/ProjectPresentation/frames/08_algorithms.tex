\begin{frame}
    \frametitle{Optimization Algorithms}

    Since neural networks have a \textit{lot} of parameters to optimize, we use variations of \textit{stochastic gradient descent} (SGD) to make the training process possible on modern hardware.

    \bigskip
    \pause

    The stochastic gradient descent algorithm uses random subsets of the training data to compute an approximation to the gradient of the residual.

    \bigskip
    \pause

    

    
\end{frame}