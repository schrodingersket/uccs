\begin{frame}
    \frametitle{Optimization Algorithms}

    Since neural networks have a \textit{lot} of parameters to train, we use variations of gradient descent to make
    the training process possible on modern hardware.

    \bigskip
    \pause

    \textbf{Adam}
    \ \\
    \bigskip
    Used to find a global minimum.

    \pause
    \bigskip

    \textbf{L-BFGS}
    \ \\
    \bigskip
    Used to find a local minimum.
\end{frame}