Neural networks are composed of affine compositions of squashing functions. For a neural network of a given size and 
depth, the variable parameters which may be optimized are the weights and biases of the network. Since each squashing 
function has its own associated weight and bias terms, the number of parameters in a neural network can be quite high; 
as a result, standard optimization algorithms which rely on full gradient calculations (with respect to each neural 
network parameter) can be computationally expensive. For this reason, we introduce the L-BFGS\cite{Liu1989-sf} and Adam
\cite{kingma_adam_2014} optimization algorithms which are designed to accommodate the large number of parameters present
in neural networks.